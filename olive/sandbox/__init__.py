# cli/olive/sandbox/__init__.py
import os
import re
import subprocess
import signal
import atexit
import hashlib
import tarfile
import datetime
import shutil
import json
import asyncio
from contextlib import contextmanager
from pathlib import Path
from tempfile import NamedTemporaryFile
from olive.logger import get_logger
from olive.preferences import prefs
from olive.env import get_olive_root, get_project_root, get_resource_path
from olive.tasks.models import TaskSpec
from olive.ui import console, console_lock
from rich.markup import escape
from .env import get_mounts, get_container_name, docker_required
from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer


logger = get_logger("sandbox")

ANSI_RE = re.compile(r"\x1b\[[0-9;]*m")


class SandboxManager:
    def __init__(self):
        self.container_name = get_container_name()
        self.log_path = Path(".olive/logs/sandbox.log")
        self.log_path.parent.mkdir(parents=True, exist_ok=True)

    def write_dockerignore(self):
        """Generate .dockerignore in .olive/sandbox and copy to project root for Docker to see."""
        sandbox_root = get_project_root() / ".olive/sandbox"
        project_root = get_project_root()
        sandbox_ignore = sandbox_root / ".dockerignore"
        project_ignore = project_root / ".dockerignore"

        logger.info(f"[sandbox] Writing .dockerignore to: {sandbox_ignore}")
        sandbox_root.mkdir(parents=True, exist_ok=True)

        header = [
            "# Generated by Olive",
            f"# Written at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "# Do not edit this file manually.",
            "# It will be overwritten on each sandbox build.\n",
        ]

        base_ignores = [
            "*.db",
            "*.log",
            "*.pyc",
            "*.sqlite3",
            "*.test.py",
            ".DS_Store",
            ".env",
            ".git/",
            ".git/**/*",
            ".venv/",
            ".venv/**/*",
            "__pycache__/",
            "legacy/unused_module.py",
            "node_modules/",
            "venv/",
            ".olive/",
        ]

        allowlist = [
            "!.olive/sandbox/.olive-snapshot",
            "!.olive/sandbox/entrypoint.sh",
            "!.olive/sandbox/staging/",
            "!.olive/sandbox/staging/olive/",
            "!.olive/sandbox/staging/olive/**",
        ]

        all_lines = header + sorted(set(base_ignores)) + allowlist
        all_lines += sorted(prefs.get("context", "exclude", "paths", default=[]))
        all_lines += sorted(prefs.get("context", "exclude", "patterns", default=[]))

        sandbox_ignore.write_text("\n".join(all_lines) + "\n")
        logger.info("[sandbox] .dockerignore written to sandbox.")

        # 🚨 Required by Docker — must live at root of build context
        shutil.copyfile(sandbox_ignore, project_ignore)
        logger.info(
            f"[sandbox] Copied .dockerignore to build context root: {project_ignore}"
        )

    @docker_required
    def build(self, force: bool = False):
        """Build the Olive sandbox Docker image (inside the user's project tree)."""

        self.write_dockerignore()

        project_root = get_project_root()
        user_root = get_olive_root() or Path.home() / ".olive"
        snapshot_dir = project_root / ".olive/sandbox/.olive-snapshot"
        backup_dir = snapshot_dir.parent / "old_snapshots"
        snapshot_dir.parent.mkdir(parents=True, exist_ok=True)
        backup_dir.mkdir(parents=True, exist_ok=True)

        template_path = get_resource_path("olive.sandbox", "Dockerfile.template")
        dockerfile_path = project_root / ".olive/sandbox/Dockerfile"

        # Hash user ~/.olive
        current_hash = self._hash_directory(user_root)
        snapshot_marker = snapshot_dir / ".snapshot_hash"

        snapshot_changed = True
        if snapshot_marker.exists() and not force:
            prev_hash = snapshot_marker.read_text().strip()
            if prev_hash == current_hash:
                snapshot_changed = False
            else:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                archive_path = backup_dir / f"snapshot_{timestamp}_{prev_hash}.tar.gz"
                with tarfile.open(archive_path, "w:gz") as tar:
                    tar.add(snapshot_dir, arcname=".olive-snapshot")
                logger.info(f"Backed up previous snapshot to: {archive_path}")

        if snapshot_changed or force:
            if snapshot_dir.exists():
                shutil.rmtree(snapshot_dir)
            shutil.copytree(user_root, snapshot_dir)
            self._disable_sandbox(snapshot_dir / "preferences.yml")
            snapshot_marker.write_text(current_hash)

        # Render Dockerfile
        rendered_dockerfile = self._render_dockerfile(template_path)
        shutil.copy(
            get_resource_path("olive.sandbox", "entrypoint.sh"),
            dockerfile_path.parent / "entrypoint.sh",
        )
        if (
            not dockerfile_path.exists()
            or dockerfile_path.read_text() != rendered_dockerfile
            or force
        ):
            dockerfile_path.write_text(rendered_dockerfile)
            logger.info(f"Dockerfile written to: {dockerfile_path}")
        else:
            logger.info("No changes to Dockerfile — skipping rewrite.")

        # Rebuild if snapshot or Dockerfile changed
        if snapshot_changed or force:
            cmd = [
                "docker",
                "build",
                "-f",
                str(dockerfile_path),
                "-t",
                "olive/sandbox:latest",
                str(project_root),
            ]

            with self._docker_spinner("Building sandbox image…") as st:
                proc = subprocess.Popen(  # noqa: S603
                    cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
                )
                assert proc.stdout  # for mypy
                for line in proc.stdout:
                    clean = ANSI_RE.sub("", line).strip()
                    if clean:
                        logger.debug("[docker build] %s", clean)
                        with console_lock():
                            st.update(f"[secondary]{escape(clean[:80])}[/secondary]")
                proc.wait()
                proc.stdout.close()

            if proc.returncode != 0:
                raise RuntimeError("🚨 Docker build failed – see log for details.")
            logger.info("✅ Sandbox image built successfully.")

        else:
            logger.info("No rebuild necessary.")

    @docker_required
    def _ensure_image(self):
        result = subprocess.run(
            ["docker", "images", "-q", "olive/sandbox:latest"],
            capture_output=True,
            text=True,
        )
        if not result.stdout.strip():
            logger.warning("[sandbox] Image not found — building...")
            self.build()

    @docker_required
    def start(self):
        """
        Start the Olive sandbox container.

        This method launches a long-lived Docker container that runs Olive in
        `--daemon shell` mode inside a Docker-managed pseudo-TTY environment.

        It passes a single argument: `'daemon'`, which triggers the corresponding
        entrypoint behavior in the container. That entrypoint script:
        - Activates the Olive virtual environment
        - Initializes the mounted project directory via `olive init`
        - Launches `olive --daemon shell`, which internally manages a tmux session

        This setup ensures:
        - A persistent, terminal-aware container that keeps the shell alive
        - Interactive shell behavior via `docker exec -it` or `:sandbox-tty`
        - Compatibility with Olive's internal tool and task dispatch loop

        Requirements:
            The container must be started with `-dit` to allocate a pseudo-TTY.
            Without a TTY, both `tmux` and `olive shell` will fail to initialize.

        Raises:
            RuntimeError: If the container exits immediately after launch.
            CalledProcessError: If `docker run` fails to start the container.
        """
        if self.is_running():
            logger.info("Sandbox already running.")
            return

        self._ensure_image()

        mode = os.getenv("SANDBOX_MODE") or prefs.get("sandbox", "disk", default="copy")
        mounts = get_mounts()
        logger.debug(f"Mount strategy: {mode}")
        logger.debug(f"Mounts for sandbox: {mounts}")

        mount_args = []
        if mode == "mount":
            for host, container, ro in mounts:
                flag = "ro" if ro else "rw"
                mount_args += ["-v", f"{host}:{container}:{flag}"]
        else:
            logger.info("Sandbox running in COPY mode.")

        cmd = [
            "docker",
            "run",
            "-dit",  # Allocate TTY (tmux and olive shell require it)
            "--name",
            self.container_name,
            *mount_args,
            "--workdir",
            "/sandbox",
            "olive/sandbox:latest",
            "daemon",  # Triggers 'daemon' branch in entrypoint.sh
        ]

        logger.info("Starting sandbox container: %s", self.container_name)
        try:
            with self._docker_spinner("Starting sandbox…") as st:
                proc = subprocess.Popen(
                    cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
                )  # noqa: S603
                assert proc.stdout
                for line in proc.stdout:
                    clean = ANSI_RE.sub("", line).strip()
                    if clean:
                        logger.debug("[docker run] %s", clean)
                        with console_lock():
                            st.update(f"[secondary]{escape(clean[:80])}[/secondary]")

                proc.wait()
                proc.stdout.close()

            if proc.returncode != 0:
                raise subprocess.CalledProcessError(proc.returncode, cmd)
            logger.info(
                "Docker reported successful container launch — validating status..."
            )

            if self.is_running():
                logger.info("✅ Sandbox container is running.")
            else:
                log_result = subprocess.run(
                    ["docker", "logs", "--tail", "20", self.container_name],
                    capture_output=True,
                    text=True,
                )
                log_snippet = log_result.stdout.strip()

                logger.warning("⚠️ Sandbox container exited immediately after launch.")
                logger.debug(f"[sandbox logs]\n{log_snippet}")
                raise RuntimeError(
                    "Sandbox container started but exited immediately. "
                    "Run `!docker ps -a` and `!docker logs <container>` for details."
                )
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to start sandbox container: {e}")
            raise

    @docker_required
    def dispatch_task(self, spec: TaskSpec, wait: bool = True) -> dict:
        """
        Dispatch a structured task into the sandbox.

        If a return_id is present, the task will be saved to disk and passed to the
        sandbox via 'olive run-task <path>'. The return_id becomes the result filename.

        Args:
            spec (TaskSpec): The task to run (includes tool name and input).
            wait (bool): Whether to wait for the result file to appear.

        Returns:
            dict: Result data (if wait=True), or dispatch metadata (if wait=False).
        """
        from olive.env import get_project_root

        # Persist task to disk if return_id is present
        if spec.return_id:
            spec.save()
            task_path = spec.path()
        else:
            # Use a temp file for ephemeral execution
            with NamedTemporaryFile(
                "w+", suffix=".json", delete=False, encoding="utf-8"
            ) as f:
                f.write(json.dumps(spec.model_dump(), indent=2))
                task_path = Path(f.name)

        # Remap host path to visible path inside the container
        relative_path = task_path.relative_to(get_project_root())
        sandbox_path = Path("/mnt/project") / relative_path

        logger.info(
            f"[sandbox] Dispatching task {spec.return_id or spec.id} → {sandbox_path}"
        )

        cmd = [
            "docker",
            "exec",
            self.container_name,
            "olive",
            "run-task",
            str(sandbox_path),
        ]

        # Capture output so it doesn’t break the Rich spinner in the host shell
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            logger.error(
                "[sandbox] Task command failed (%s): %s",
                result.returncode,
                result.stderr.strip(),
            )
            raise subprocess.CalledProcessError(
                result.returncode, cmd, result.stdout, result.stderr
            )

        # Still keep the data in DEBUG logs for diagnostics
        if result.stdout.strip():
            logger.debug("[sandbox stdout]\n%s", result.stdout.strip())
        if result.stderr.strip():
            logger.debug("[sandbox stderr]\n%s", result.stderr.strip())

        if not wait or spec.return_id is None:
            return {
                "dispatched": True,
                "task_id": spec.id,
                "return_id": spec.return_id,
                "result_path": str(spec.result_path()),
            }

        # await result file via watchdog instead of busy‑poll
        result_path = spec.result_path()
        logger.info("[sandbox] Waiting for result via watchdog: %s", result_path)

        class _Done(FileSystemEventHandler):  # noqa: D401 – inner class
            def __init__(self):
                self._ready = asyncio.Event()

            def on_created(self, event):  # type: ignore[override]
                if Path(event.src_path) == result_path:
                    self._ready.set()

        handler = _Done()
        observer = Observer()
        observer.schedule(handler, result_path.parent, recursive=False)
        observer.start()
        try:
            while not result_path.exists():
                if handler._ready.wait(1):  # wait 1 s chunks so we can Ctrl‑C
                    break
        finally:
            observer.stop()
            observer.join()

        logger.info("[sandbox] ✅ Result ready: %s", result_path)
        return json.loads(result_path.read_text())

    async def dispatch_task_async(self, spec: TaskSpec) -> dict:
        """
        Async version of dispatch_task that doesn't block the event loop.

        This runs in a background thread and returns the same result.
        """
        return await asyncio.to_thread(self.dispatch_task, spec)

    @docker_required
    def dispatch_tool(self, toolname: str, args: list[str]) -> dict:
        """
        Send a tool invocation into the sandbox shell via tmux.

        Formats the call as `!!toolname args...` and sends it to the running Olive
        shell inside the container (via `docker exec + tmux send-keys`). This is
        used when tool calls (e.g. from LLM XML) are routed through the sandbox.

        Args:
            toolname: Name of the Olive tool to invoke.
            args: List of CLI-style arguments for the tool.

        Returns:
            dict with stdout, stderr, and return code.

        Raises:
            RuntimeError if sandbox is not running or dispatch fails.
        """
        if not self.is_running():
            raise RuntimeError("Sandbox is not running.")

        from shlex import quote

        quoted_args = " ".join(map(quote, args))
        shell_expr = f"!!{toolname} {quoted_args}".strip()

        logger.info(f"[sandbox] Sending to sandbox daemon via tmux: {shell_expr}")

        cmd = [
            "docker",
            "exec",
            self.container_name,
            "tmux",
            "send-keys",
            "-t",
            "default",
            shell_expr,
            "C-m",
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError(
                f"Failed to send command to sandbox daemon: {result.stderr.strip()}"
            )

        return {
            "stdout": f"Dispatched to sandbox daemon: {shell_expr}",
            "stderr": result.stderr.strip(),
            "returncode": result.returncode,
        }

    def _render_dockerfile(self, template_path: Path) -> str:
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S %Z")
        banner = f"""\
# ─────────────────────────────────────────
# 🚧 AUTOGENERATED DOCKERFILE
# Generated by Olive at: {now}
# DO NOT EDIT — see Dockerfile.template
# ─────────────────────────────────────────
"""
        project_root = get_project_root().resolve()
        olive_module_root = get_resource_path("olive").resolve().parent

        try:
            # Try to compute path relative to build context
            olive_source_path = olive_module_root.relative_to(project_root)
        except ValueError:
            # Fallback: copy into .olive/sandbox/staging
            staging_path = project_root / ".olive/sandbox/staging/olive"
            logger.warning(
                f"[sandbox] Olive source is outside build context. Copying to: {staging_path}"
            )

            # ✅ Ensure parent directory exists
            staging_path.parent.mkdir(parents=True, exist_ok=True)

            try:
                if staging_path.exists():
                    shutil.rmtree(staging_path)
                shutil.copytree(olive_module_root, staging_path)
            except Exception as e:
                raise RuntimeError(
                    f"Failed to copy Olive source to sandbox staging: {e}"
                )

            olive_source_path = staging_path.relative_to(project_root)

        body = (
            template_path.read_text()
            .replace("{{ olive_user }}", "olive")
            .replace("{{ olive_source_path }}", str(olive_source_path))
            .replace("{{ olive_prefs_snapshot }}", ".olive/sandbox/.olive-snapshot")
        )
        return f"{banner}\n{body.strip()}\n"

    def _hash_directory(self, path: Path) -> str:
        hasher = hashlib.sha1()
        for p in sorted(path.rglob("*")):
            if p.is_file():
                hasher.update(p.read_bytes())
        return hasher.hexdigest()

    def _disable_sandbox(self, prefs_path: Path):
        try:
            import yaml

            data = yaml.safe_load(prefs_path.read_text())
            data["sandbox"]["enabled"] = False
            prefs_path.write_text(yaml.safe_dump(data))
        except Exception as e:
            logger.warning(f"Could not disable sandbox in snapshot prefs: {e}")

    def is_running(self) -> bool:
        try:
            result = subprocess.run(
                ["docker", "ps", "-q", "-f", f"name={self.container_name}"],
                capture_output=True,
                text=True,
            )
            return bool(result.stdout.strip())
        except FileNotFoundError:
            logger.warning("[sandbox] Docker not found — assuming sandbox not running.")
            return False

    @docker_required
    def stop(self):
        if not self.is_running():
            logger.info("Sandbox not running.")
            return
        logger.info(f"Stopping sandbox container: {self.container_name}")
        subprocess.run(
            ["docker", "rm", "-f", self.container_name], stdout=subprocess.DEVNULL
        )
        logger.info("Sandbox stopped.")

    def restart(self):
        self.stop()
        self.start()

    @docker_required
    def status(self) -> dict:
        if not self.is_running():
            return {"running": False}

        result = subprocess.run(
            [
                "docker",
                "stats",
                "--no-stream",
                "--format",
                "{{.Name}} {{.CPUPerc}} {{.MemUsage}}",
            ],
            capture_output=True,
            text=True,
        )

        for line in result.stdout.strip().splitlines():
            if line.startswith(self.container_name):
                _, cpu, mem = line.split(maxsplit=2)
                return {
                    "running": True,
                    "cpu": cpu,
                    "mem": mem,
                    "name": self.container_name,
                }
        return {"running": True, "name": self.container_name}

    # ────────────────────────────────────────────────────────────
    # private helper: run long docker cmd with live spinner
    # ────────────────────────────────────────────────────────────
    @contextmanager
    def _docker_spinner(self, title: str):
        """
        Context‑manager yielding a Rich Status object (`st`); all updates
        should be wrapped in `with console_lock():` if called from a background
        thread.
        """
        with console.status(f"[highlight]{title}[/highlight]", spinner="dots") as st:
            yield st

    @docker_required
    def logs(self, tail=40, follow=False):
        if not self.is_running():
            print("[yellow]Sandbox is not running.[/yellow]")
            return
        cmd = ["docker", "logs", "--tail", str(tail)]
        if follow:
            cmd.append("--follow")
        cmd.append(self.container_name)
        subprocess.run(cmd)


sandbox = SandboxManager()
atexit.register(sandbox.stop)
signal.signal(
    signal.SIGTERM,
    lambda signum, frame: logger.info(
        f"[sandbox] Caught SIGTERM (signal {signum}), stopping sandbox."
    )
    or sandbox.stop(),
)
signal.signal(
    signal.SIGINT,
    lambda signum, frame: logger.info(
        f"[sandbox] Caught SIGINT (signal {signum}), stopping sandbox."
    )
    or sandbox.stop(),
)
